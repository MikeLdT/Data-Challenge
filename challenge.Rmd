---
title: "GEN Z, sin miedo a invertir"
author: "Miguel Lerdo de Tejada, Anahí Plascencia, Alejandro Gómez, Alejandro Ortiz"
date: "`r Sys.Date()`"
output:
  prettydoc::html_pretty:
    theme: architect
    highlight: github
    math: katex
---
<style>
body {
text-align: justify}

#TOC {
  font-family: brandon;
  font-size: 16px
  border-color: dodgerblue4
  background: #c8e4f6 
  color: #15025e 
  font: brandon 
}
body {
  color: dodgerblue4
  font-family: brandon;
  background-color:gray94 ;
}

</style>

# El COVID-19 ha sido un impulso para que los más jóvenes inviertan
La generación Z ha demostrado que nunca es demasiado pronto para empezar a invertir, desde que comenzó la pandemia en marzo de 2020 muchos jóvenes al rededor del mundo han decidido tomar las riendas de sus finanzas personales. Muchos quizá por aburrimiento, y otros por la frustración de lo que está pasando con la economía mundial han decidido intentarlo y los resultados han sido bastante interesantes. 

La facilidad con la que uno puede comprar acciones con un simple swipe a la izquierda es fascinante. El mundo digital ha venido a revolucionar los mercados y la Gen Z no perderá ninguna oportunidad para hacerlo desde casa. 

Por esta razón hemos decidido analizar los efectos del lockdown en Estados Unidos para descubrir si existe un efecto causal en la tendencia de búsqueda de palabras relacionadas al mercado accionario utilizando Google Trends. Buscamos probar que debido a los confinamientos el número de búsquedas relacionadas con inversiones han aumentado. 

```{r setup, include=F}
#options(repos="https://cran.rstudio.com" )
knitr::opts_chunk$set(echo = TRUE, comment = NA, message = FALSE)
```


```{r, include=FALSE, message=FALSE, warning=FALSE, error=FALSE}

library("dplyr")
library("ggplot2")
library("corrplot")
library("gtrendsR")
library(readr) 
library(purrr) 
library(crossmap)
library(lubridate)
library(fixest)
library(naniar)
library(future)
library(rdrobust)
library(fastDummies)
library(rddensity)
library(ggstatsplot)
library(gridExtra)
library("rmarkdown")
library("naniar")
library("knitr")
library("furrr")
library("prettydoc")
library(kableExtra)

```
Los resultados que obtuvimos se estimaron a través del método de Diferencias en Diferencias _(DiD)_ que compara las búsquedas relacionadas con finanzas en el 2020 antes y después de los lockdowns con las búsquedas en las mismas fechas en el 2019. Los datos que se tomaron fueron para los 51 estados de Estados Unidos y la selección de palabras de términos de búsqueda relacionadas a la categoría de finanzas en Google Trends fueron las siguientes: Stock, Stocks, Market, NYSE, Finance, Finances, Dow, NASDAQ, Business, Rate, Rates y S&P. Los datos de búsqueda por cada término se registraron diariamente para cada estado en ambos periodos de tiempo. 

De acuerdo con los resultados encontrados nos podemos dar cuenta que a partir de la pandemia la gente empezó a tener un interés por las finanzas que se ve reflejado en como los términos de búsqueda aumentaron a partir del lockdown. (Incluir los datos en términos de desviaciones). En el estado de blah se vio un aumento de ... desviaciones estándar. Tal término aumentó --- veces. Mencionar qué pasó con los términos más relevantes y cómo aumentó su búsqueda por el lockdown. 

Para obtener las tendencias de búsqueda diarias entre el 1 de enero de 2019 y el 10 de abril de 2020, descargamos los datos diarios entre el 1 de enero y el 10 de abril, tanto en 2019 como en 2020. Como los datos diarios en 2019 provienen de una solicitud separada a los datos diarios en 2020, los factores de escala utilizados para calcular la puntuación de 0 a 100 no son los mismos en los dos períodos. Por lo tanto, cambiamos la escala de las dos series para que sean comparables.

## Procedimiento de escala:

Denotemos con $D_{i,c,2019}$ el número de búsquedas diarias en Google de un tema el día $i$ en el estado $c$, durante el período del 1 de enero de 2019 al 10 de abril de 2019, con un número análogo $D_{i,c,2020}$ para el período del 1 de enero de 2020 al 10 de abril de 2020. Estos datos se obtienen para cada día individual $i$ y toman valores entre 0 y 100 para cada día durante el período considerado. Sin embargo, no se puede comparar directamente los números de 2019 y 2020 ya que su denominador (el número máximo de búsquedas durante un día en el período) no es el mismo. Para poder comparar estas cifras, cambiamos la escala de los datos diarios para cada período por las respectivas ponderaciones de interés de búsqueda de la semana que calculamos utilizando datos semanales que están disponibles continuamente durante todo el periodo entre el 1 de enero de 2019 y el 10 de abril de 2020.

Denotemos por $D_{i,c,2019-2020}$ el número reescalado de búsquedas diarias en Google para este tema el día $i$ en el estado $c$ durante el período del 1 de enero de 2019 al 10 de abril de 2020. Este factor lo calculamos de la siguiente manera:

Primero calculamos los respectivos niveles de busqueda semanal de los temas de nuestro interés para todas las semanas entre el 1 de enero de 2019 y el 10 de abril de 2020. Tomamos los datos diarios desde el 1 de enero de 2019 hasta el 10 de abril de 2019 y calculamos el promedio de búsquedas semanales para cada tema en el país $c$ sobre este periodo y lo denotamos $D_{i,c,2019}$. Luego realizamos el mismo ejercicio para el periodo del 1 de enero de 2020 al 10 de abril de 2020 y lo denotamos:  $D_{i,c,2020}$.
De igual manera sacamos el promedio del nivel de busqueda de cada tema para todo el periodo (es decir, desde el 1 de enero de 2019 hasta el 10 de abril de 2020), también lo denotamos: $D_{i,c,2019-2020}$ 

De lo anterior ponderamos el nivel de las búsquedas semanales, $w_{c,2019}$ y $w_{c,2020}$:
$w_{c,2019}= \frac{\overline{D_{i,c,2019-2020}}}{D_{i,c,2019}}$ y $w_{c,2020}= \frac{\overline{D_{i,c,2019-2020}}}{D_{i,c,2020}}$
 

Con estas ponderaciones ahora podemos cambiar la escala de los datos diarios para cada período multiplicando $D_{i,c,2019}$ por $w_{c,2019}$ en 2019 y $D_{i,c,2020}$ por $w_{c,2020}$ en 2020.
Obtenemos:
$$D_{i,c,2019-2020}=D_{i,c,2019}*\frac{\overline{D_{i,c,2019-2020}}}{D_{i,c,2019}}$$ para 2019
$$D_{i,c,2019-2020}=D_{i,c,2020}*\frac{\overline{D_{i,c,2019-2020}}}{D_{i,c,2020}}$$ para 2020


Por último, normalizamos estas cifras para obtener cifras entre 0 y 100 remplazando $D_{i,c,2019-2020}$ por:
$$D_{i,c,2019-2020}=\frac{D_{i,c,2019-2020}}{max(D_{i,c,2019-2020})}*100$$

 


## Selección de la muestra:


Utilizamos la informsción que Google Trends nos proporciona sobre el historial de busqueda de las personas en Estados Unidos, no solo porque fue uno de los paises que con mayor rapidez impusieron restricciones de confinamiento, sino también porque la cultura de la inversión digital en la bolsa de valores es mucho más común, porque una mayor cantidad de personas tienen acceso a internet. De esta manera suavizamos un poco el sesgo que podría existir en nuestra muestra por la exclusión de personas que no tienen acceso a internet para nuestro ejercicio. Este sesgo es una de las desventajas que tiene utilizar Google Trends como fuente de datos, así como el sesgo por edades, porque es mucho más común que la gente joven utilice internet para buscar maneras de invertir en la bolsa, mientras la gente de edad lo puede hacer por otro medio. Esto podría excluir de nuestro ejercicio a muchas personas mayores.

Sin embargo, utilizar Google Trends en nuestro trabajo tiene también ventajas; primero, las muestras que obtenemos son grandes, por lo que se elimina el sesgo por muestras pequeñas. Segundo, no adolece de sesgos como el efecto expectativa del observador o el sesgo del entrevistador, porque los usuarios no tienen incentivos a mentir en sus búsquedas privadas. Y tercero, no hay una submuestra de entrevistados, por lo que no participa en nuestro ejercicio gente auto informada.

## Método:

Para estimar los efectos del confinamiento en las búsquedas relacionadas con el sector financiero, nos basamos en una estimación de Diferencia en Diferencia (DiD) que compara las búsquedas antes y después de la cuarentena en 2020 con las búsquedas anteriores y posteriores a la misma fecha en 2019, esto garantiza que los cambios estacionales no afectan nuestro ejercicio porque comparamos las mismas fechas, en diferentes años. 

Escribimos el modelo de regresión de diferencias en diferencias para un término $W$ como:
$$W_{i,c}=\alpha T_{i,c}*Year_{i}+\beta T_{i,c}+\mu_{i}+\rho_{c}+\epsilon_{i,c}$$
 

Donde $\alpha$ refleja el efecto del _lockdown_ en las búsquedas de Google para el término $W_{i, c}$ en el día $i$ en el estado $c$. $T_{i,c}$ es una variable dummy que toma el valor uno en los días posteriores a que se anunció el confinamiento y es cero en fechas anteriores. El año del _lockdown_ es el año $i$ y corresponde a 2020. La variable $X_{i-1,c}$ controla el número de nuevas muertes por COVID-19 por día por cada millón de habitantes en el estado $c$. El modelo incluye efectos fijos del estado, $\rho_{c}$, así como efectos fijos de año, semana y día que aparecen en el vector $\mu$. 

Los errores estándar son sólidos y están agrupados a nivel de día. La suposición clave en nuestro ejercicio es que, en ausencia del confinamiento, el comportamiento de los usuarios de Google habría evolucionado de la misma manera que en el año anterior al _lockdown_, es decir, una suposición de tendencia común.


## Estimadores RDD (Regresión discontinua)

Para mostrar que hay una ruptura estructural inmediata causada por el confinamiento en las tendencias de búsqueda de inversiones, también utilizaremos el método de regresión discontinua (RDD), que identifica rupturas en dos series paramétricas estimadas antes y después del confinamiento. Al igual que con las estimaciones de DiD, comparamos estas rupturas con las estimadas durante el mismo período en 2019.

Sea $D$ la variable de ejecución, que se define como la distancia absoluta en días desde el anuncio de la orden de confinamienro; es negativo para los días anteriores y positivo para los días posteriores, mientras que la fecha del anuncio real o contrafactual se establece como día cero. El anuncio de bloqueo $T_{i,c}$ se define como ya lo habíamos definido anteriormente. Por lo tanto, el modelo RDD-DiD se puede escribir de la siguiente manera:
$$\small W_{i,c}=\alpha\prime T_{i,c}*Year_{i}+ \psi f(D_{i,c})*Year_{i}+\theta f(D_{i,c})(1-T_{i,c})*Year_{i}+\phi f(D_{i,c})*(1-T_{i,c}) $$
 
$$+\beta\prime T_{i,c} + \gamma X_{i-1,c} + \mu\prime_{i}+ \rho\prime_{c}+ \epsilon\prime_{i,c}$$

Donde $\alpha\prime$ refleja el efecto que causó el confinamiento en las búsquedas de Google del término $W_i$, $c$ en el día $i$ en el estado $c$. $f(D_{i,c})$ es una función polinomial de la distancia en días desde el anuncio del confinamiento que interactuó con la variable dummy de _lockdown_ $T_{i,c}$, para permitir diferentes efectos en ambos lados de las regresiones (antes y después). Además, se incluyeron los mismos controles que en los modelos DID.

```{r, echo=FALSE, message=FALSE, warning=FALSE, error=FALSE}
#cargo los ids de los estados y DC
data("countries")
edos <- countries %>% 
  filter(country_code=="US") %>% 
  filter(!is.na(sub_code)) %>% 
  select(sub_code) %>% 
  pull() %>% 
  unique() %>% 
  head(51)

# Load your keywords list (.csv file) 
#cat("\n", file= file.choose(), append = TRUE)
kwlist <- readLines("palabras.csv")

#cargo las fechas de lockdown y les pego el identificador de cada edo
fechas <- read.csv("fechasperronas.csv")
fechas$State <- toupper(fechas$State)
fechas <- fechas %>% 
  rename(name=State)

counts <- countries %>% 
  filter(country_code=="US") %>% 
  filter(!is.na(sub_code)) %>% 
  head(51)

fechas_join <- left_join(x=fechas,y=counts,by=c("name")) %>% 
  rename(geo=sub_code)

#jalo las muertes y las atraso un dia
muertes <- read.csv("all-states-history.csv") %>% 
  select(date,state,death) %>% 
  mutate(state=paste0("US-",state),date=as.Date(date)+1) %>% 
  rename(geo=state)

#poblacion
pob <- read.csv("population2019.csv") %>% 
  mutate(Population=as.numeric(gsub(",","",Population)))
pob$States <- toupper(pob$States)
pob <- pob %>% 
  rename(name=States) %>% 
  filter(!is.na(Population)) %>% 
  left_join(x=.,y=counts,by=c("name")) %>% 
  rename(geo=sub_code) %>% 
  select(geo,Population)

```


```{r bajarPalabras, eval=F, cache=TRUE, echo=FALSE, message=FALSE, warning=FALSE, error=FALSE}


kwlist <- kwlist[! kwlist==""]

 
# The function wrap all the arguments of the gtrendR::trends function and return only the interest_over_time (you can change that)
googleTrendsData <- function (keywords,country,time) { 
  
  # Set the geographic region, time span, Google product,... 
  # for more information read the official documentation https://cran.r-project.org/web/packages/gtrendsR/gtrendsR.pdf 
  #keywords <- kwlist
  #country <- "US" 
  channel <- 'web' 
  
  trends <- gtrends(keywords, 
                   gprop = channel,
                   geo = country,
                   time = time,
                   category=7) 
    
  Sys.sleep(7)
  
  results <- trends$interest_over_time 
  results$hits <- as.character(results$hits)
  return(results)
  } 
date <- c("2019-01-01 2019-04-10","2020-01-01 2020-04-10") 

# googleTrendsData function is executed over the cross product of kwlist,edos and date for daily data
# and the weekday and week variables are created

output <- future_xmap_dfr(.l = list(kwlist,edos,date),
                  .f = ~ googleTrendsData(..1,..2,..3),
                  .progress = T, .options = furrr_options(seed=NULL)) %>% 
  mutate(weekday=wday(as.Date(date)),week=week(as.Date(date)))
  
 
# Download the dataframe "output" as a .csv file 
write.csv(output, "download_diarias1.csv")

date <- c("2019-01-01 2020-04-10")

# googleTrendsData function is executed over the cross product of kwlist,edos and date for daily data
# and the weekday and week variables are created
output_week <- future_xmap_dfr(.l = list(kwlist, edos,date),
                  .f = ~ googleTrendsData(..1,..2,..3),
                  .progress = T,.options = furrr_options(seed=NULL)) 
 
# Download the dataframe "output" as a .csv file 
write.csv(output_week, "download_semanal1.csv")
```

```{r rescaling, echo=FALSE, message=FALSE, warning=FALSE, error=FALSE}
output <- read.csv("download_diarias.csv") %>% 
  mutate(year=year(date))
output_week <- read.csv("download_semanal.csv") %>% 
  mutate(year=year(date))



#convierto los hits de character a numeric
output$hits <- as.numeric(output$hits)
output <- output %>% 
  mutate(hits=ifelse(is.na(hits),0,hits))
output_week$hits <- as.numeric(output_week$hits)

# calculo el interes promedio por palabra del periodo 2019-10 abril 2020
period_mean <- output_week %>% 
  group_by(keyword,geo) %>%
  summarise(period_mean=mean(hits),.groups="keep") %>% 
  ungroup() 

# junto el calculo anterior con los datos
output <- left_join(x=output,y=period_mean,by=c("keyword","geo"))

# calculo el promedio por semana para los datos diarios por estado y palabra
# luego hago los pesos y corrijo y reescalo los hits
week_share <- output %>% 
  mutate(year=year(date)) %>% 
  group_by(keyword,geo,week,year) %>% 
  mutate(weights=period_mean/mean(hits)) %>% 
  mutate(hits_aux=hits*weights) %>% 
  mutate(hits_corrected=100*hits_aux/max(hits_aux)) %>% 
  mutate(hits_corrected=ifelse(is.na(hits_corrected),0,hits_corrected)) %>% 
  ungroup()





```


```{r treatment,echo=FALSE, message=FALSE, warning=FALSE, error=FALSE}

# a cada estado le pego la fecha en que anunciarion, entro en vigor y empezo en la primera ciudad el lockdown
# y vienen en otro formato entonces se los cambio

week_share_join <- left_join(x=week_share,y=fechas_join,by=c("geo"))
week_share_join$date <- as.Date(week_share_join$date)
week_share_join$Lockdown.announced <- as.Date(week_share_join$Lockdown.announced,"%d/%m/%Y")
week_share_join$Lockdown.effective <- as.Date(week_share_join$Lockdown.effective,"%d/%m/%Y")
week_share_join$X1st.city.county.lockdown.effective <- as.Date(week_share_join$X1st.city.county.lockdown.effective,"%d/%m/%Y")

# creo las variables de si estan despues del lockdown o no

week_share_join <- week_share_join %>% 
  mutate(treat_announced=ifelse(date>=Lockdown.announced,1,0))%>% 
  mutate(treat_announced=ifelse(is.na(treat_announced),0,treat_announced)) %>% 
  mutate(treat_effective=ifelse(date>=Lockdown.effective,1,0))%>% 
  mutate(treat_effective=ifelse(is.na(treat_effective),0,treat_effective)) %>%
  mutate(treat_1stcity=ifelse(date>=X1st.city.county.lockdown.effective,1,0)) %>% 
  mutate(treat_1stcity=ifelse(is.na(treat_1stcity),0,treat_1stcity))
  




```

```{r muertes,echo=FALSE, message=FALSE, warning=FALSE, error=FALSE}

# le pego las muertes y la poblacion a la base

base_final <- left_join(x=week_share_join,y=muertes,by=c("date","geo")) %>% 
  mutate(death=ifelse(is.na(death),0,death)) %>%
  mutate(year=year(date)) %>% 
  left_join(x=.,y=pob,by=c("geo")) %>% 
  group_by(date,keyword) %>% 
  mutate(sumPop=sum(Population)) %>% 
  ungroup() %>% 
  mutate(weightsPop=Population/sumPop)
  
  





```
### Resultados del _DiD_


```{r dif,echo=FALSE, message=FALSE, warning=FALSE, error=FALSE}
#base_final <- read.csv("base_final.csv")

#corro el dif in dif para cada palabra de la lista

difs <- map(kwlist,function(x){
  dif <- feols(hits_corrected ~ death + i(treat_announced,year,2019) | weekday + geo + week + year,filter(base_final,keyword==x),cluster=~date,weights = ~weightsPop)
  
  return(dif)
  }
  )


#fixest::coefplot(difs[12])

plotlist <- map(1:12,
                function(x){
                  
                  ggcoefstats(difs[[x]],title=kwlist[x],exclude.intercept = T,conf.level = 0.9)
                  
                })

#plotlist[[12]]

#png(filename="pruebaComb.png")
#combine_plots(
#  plotlist = plotlist[1:2] ,
#  labels = "auto",
#  annotation.args = list(title = "Efecto del lockdown")
#)

#dev.off()
#plotlist[[12]]

#a <- gridExtra::grid.arrange(grobs = plotlist[1:4])
#a
#ggcoefstats(difs[[12]],.name_repair="dif")
```

```{r, figures-side, fig.show="hold", out.width="50%",echo=T}
#plotlist
for(i in 1:12){
  plotlist[[i]]
}

```

Esta tabla muestra la estimación de diferencias en diferencias. En este modelo la variable dependiente es hits_corrected, que se refiere al nivel de búsqueda que hay de cierta palabra, en este caso las selecionadas, en un día. La escala de estos niveles está en un rango de 0 a 100. El modelo incluye como controles una variable dummy que toma el valor 1 en los días posteriores al anuncio del confinamiento y 0 en los días anteriores, así como los efectos fijos de los estados, año, semana, día y el número de nuevas muertes con retraso de un día. Los errores estándar están entre paréntesis. Los errores estándar se agrupan a nivel de día. 
Los resultados de este modelo nos idican que _ceteris paribus_, el aumento de una muerte en un día está relacionado con la disminución de 0.0016 puntos en el nivel de búsqueda de las palabras que consideramos relevantes. Lo que podría significarse que conforme se agravó más la pandemia, las personas se interesaban menos en buscar temas relacionados con inversiones, o podría ser que simplemente muchos desistían cuando empeoraba la pandemia. También, podría relacionarse con la incertidumbre que causaba en el país el aumento de las muertes por COVID-19.



### Efectos del lockdown estimado por _DiD_ para los términos_Stock_, _Stocks_, _Market_, _NYSE_, _Finance_ y _Dow Joes_
```{r,results='asis', echo=FALSE, message=FALSE, warning=FALSE, error=FALSE}
a <- etable(difs[1:6],tex = F,cluster= ~date,
            digits = 2,drop = "death", depvar = F,dict=c(treat_announced="efecto",year="lockdown"),
            extraline = list("_Weights by State" = 
                          c("Yes", "Yes", "Yes", "Yes", "Yes", "Yes"),
                          "_FE by COVID Deaths" = 
                          c("Yes", "Yes", "Yes", "Yes", "Yes", "Yes")),sdBelow = T)
colnames(a) <- kwlist[1:6]
#kable(a,)

a %>% 
kable(align = c("c", "c", "c","c", "c", "c"), caption="Efecto del lockdown en las búsquedas", digits=3,format = "html") %>%
  kable_classic(full_width = F, html_font = "Cambria") %>%
  row_spec(0, bold = T)
```
### Efectos del lockdown estimado por _DiD_  para los términos _Nasdaq_, _Finances_, _Business_, _Rate_, _Rates_ y _S&P_
```{r,results='asis', echo=FALSE, message=FALSE, warning=FALSE, error=FALSE}
a <- etable(difs[7:12],tex = F,cluster= ~date,
            digits = 2,drop = "death", depvar = F,dict=c(treat_announced="efecto",year="lockdown"),
            extraline = list("_Weights by State" = 
                          c("Yes", "Yes", "Yes", "Yes", "Yes", "Yes"),
                          "_FE by COVID Deaths" = 
                          c("Yes", "Yes", "Yes", "Yes", "Yes", "Yes")))
colnames(a) <- kwlist[7:12]
#kable(a,format = "html")

a %>% 
kable(align = c("c", "c", "c","c", "c", "c"), digits=3,format = "html") %>%
  kable_classic(full_width = F, html_font = "Cambria") %>%
  footnote(general = "Signif. codes: *** p<0.001 ** p<0.01 * p<0.05 ",threeparttable = T, footnote_as_chunk = T) %>%
  row_spec(0, bold = T)

#mean(pull(select(filter(base_final,keyword=="nasdaq",year==2019),hits_corrected)))



```
Estas dos tablas son los resultados del modelo de _Diferencias en Diferencias_ que se realizó para cada una de las palabras que consideramos relevantes. Cada columna describe un modelo para cada término. Los modelos incluyen controles para una variable dummy que toma el valor 1 en los días posteriores al confinamiento y 0 en los días anteriores, así como efectos fijos de estado, semana y día. Los errores estándar están entre paréntesis. Los errores estándar se agrupan a nivel de día. Recordemos que la forma en la que se midieron las busquedas realizadas para cada palabra fue en una escala de niveles que van del 0 al 100 para hacer comparables los años 2019 y 2020. 

Para nuestro modelo con la palabra _"stock"_ podemos concluir que el confinamiento, _ceteris paribus_, aumentó en promedio 0.92 puntos el nivel de busquedas para esta palabra. 

Para el modelo con la palabra _"stocks"_ el cambio causado por el confimaniento sí fue más significativo. _Ceteris paribus_, el hecho de estar en confimaniento está relacionado con un aumento en 5.8 puntos en promedio del nivel de búsquedas realizadas en Google Trends. Cabe resaltar que fue la segunda palabra con mayor aumento en nuestros modelos. 

En en modelo que considera la palabra _"market"_ se observa una caída en el nivel de búsquedas, y el resutado es el siguiente: _Ceteris paribus_, el hecho de estar en confinamiento está relacionado con una disminución en promedio de 0.61 puntos en el nivel de búsquedas realizadas en Google Trends. La caída promedio no fue muy grande en las búsquedas, casi podríamos considerar que se quedó constante. 

El modelo que se realizó con las búsquedas de la palabra _"NYSE"_ reporta un aumento considerable en el nivel de búsqueda. _Ceteris paribus_, el hecho de entrar al confinamiento esá relacionado con un aumento de 4.9 puntos en promedio en el nivel de búsquedas de la palabra en Google Trends. Es la tercera palabra con mayor aumento en todos lod modelos que hicimos. 

En el modelo que consideró a la palabra _"finace"_ tambíen reportó una disminución en el nivel de búsquedas. En este caso, _ceteris paribus_, el hecho de entrar en confinamiento está relacionado con una disminución en promedio de 0.06 puntos en el nivel de búsquedas de Google Trends. Es una disminución mínima, por lo que podemos considerar que no es significativa.

En el modelo que consideró a la palabra _"business"_ tambíen reportó una disminución en el nivel de búsquedas. En este caso, _ceteris paribus_, el hecho de entrar en confinamiento está relacionado con una disminución en promedio de 0.55 puntos en el nivel de búsquedas de Google Trends. Es una disminución mínima, por lo que podemos considerar que no es significativa.

De igual forma el modelo que consideró la palabra _"rate"_ muestra una caída en el nivel de búsquedas. En este caso, _ceteris paribus_, el hecho de entrar en confinamiento está relacionado con una disminución en promedio de 0.75 puntos en el nivel de búsquedas de Google Trends. De igual forma es una disminución poco considerable. 

Sin embargo, el modelo para la palabra _"rates_" sí reporta un pequeño aumento. _Ceteris paribus_, el hecho de entrar en confinamiento, está relacionado con un aumento de 1.1 puntos en el nivel de búsquedas promedio de esta palabra en Google Trends. 

Por último, esta el modelo para la palabra _"S&P_" que reporta el mayor incremento de todos los demás modelos. En este caso se reportó que _ceteris paribus_, el hecho de entrar en confinamiento está relacionado con un aumento de 7.4 puntos en el nivel promedio de búsquedas de estas palabras. 

### Resultados Regresión Discontinua
```{r rddahorasi}
#cambiar la base
base_final_rdd <- base_final %>% 
  filter(!is.na(Lockdown.announced)) %>% 
  mutate(duration=ifelse(year==2020,as.integer(date-Lockdown.announced),as.integer(date-Lockdown.announced)+365))



#covs <- cbind.data.frame(stock_2020$death,stock_2020[,(ncol(base_final)+1):ncol(stock_2020)])

rdd <- map(kwlist,function(x){
  feols(hits_corrected ~ death + i(treat_announced,year,2019) +
     duration*i(treat_announced,year,2019) +
    duration*i((1-treat_announced),year,2019) - year +
    treat_announced*duration  +
   (1-treat_announced)*duration - duration  
      | weekday + geo + week + year,filter(base_final_rdd,keyword==x,duration!=0),cluster=~date,weights = ~weightsPop,)
}
  )
```



```{r rdd,echo=FALSE, message=FALSE, warning=FALSE, error=FALSE}
#plot

rddplots_2020_apendix <- map(kwlist,function(x){
  stock_2020 <- base_final_rdd %>% 
  filter(keyword==x,year==2020)
  
b <- rdplot(y=stock_2020$hits_corrected,x=stock_2020$duration,c=0)

rdd_plots <- rddensity(X=stock_2020$duration)
appendix <- rdplotdensity(rdd_plots,X=stock_2020$duration)


})

rddplots_2019 <- map(kwlist,function(x){
  stock_2020 <- base_final_rdd %>% 
  filter(keyword==x,year==2019)
  
b <- rdplot(y=stock_2020$hits_corrected,x=stock_2020$duration,c=0)


})





```

### Efectos del lockdown estimado por _RDD_ para los términos_Stock_, _Stocks_, _Market_, _NYSE_, _Finance_ y _Dow Joes_
```{r,results='asis', echo=FALSE, message=FALSE, warning=FALSE, error=FALSE}
a <- etable(rdd[1:6],tex = F,cluster= ~date,
            digits = 2, drop="duration", depvar = F,dict=c(treat_announced="efecto",year="lockdown"),
            extraline = list("_Weights by State" = 
                          c("Yes", "Yes", "Yes", "Yes", "Yes", "Yes"),
                          "_FE by COVID Deaths" = 
                          c("Yes", "Yes", "Yes", "Yes", "Yes", "Yes")),sdBelow = T)
colnames(a) <- kwlist[1:6]
#kable(a,)

a %>% 
kable(align = c("c", "c", "c","c", "c", "c"), caption="Efecto del lockdown en las búsquedas", digits=3,format = "html") %>%
  kable_classic(full_width = F, html_font = "Cambria") %>%
  row_spec(0, bold = T)
```
### Efectos del lockdown estimado por _RDD_  para los términos _Nasdaq_, _Finances_, _Business_, _Rate_, _Rates_ y _S&P_
```{r,results='asis', echo=FALSE, message=FALSE, warning=FALSE, error=FALSE}
a <- etable(difs[7:12],tex = F,cluster= ~date,
            digits = 2,drop="duration", depvar = F,dict=c(treat_announced="efecto",year="lockdown"),
            extraline = list("_Weights by State" = 
                          c("Yes", "Yes", "Yes", "Yes", "Yes", "Yes"),
                          "_FE by COVID Deaths" = 
                          c("Yes", "Yes", "Yes", "Yes", "Yes", "Yes")))
colnames(a) <- kwlist[7:12]
#kable(a,format = "html")

a %>% 
kable(align = c("c", "c", "c","c", "c", "c"), digits=3,format = "html") %>%
  kable_classic(full_width = F, html_font = "Cambria") %>%
  footnote(general = "Signif. codes: *** p<0.001 ** p<0.01 * p<0.05 ",threeparttable = T, footnote_as_chunk = T) %>%
  row_spec(0, bold = T)

#mean(pull(select(filter(base_final,keyword=="nasdaq",year==2019),hits_corrected)))



```


## Conclusiones 

Quizá en los resultados no se aprecia en todas las palabras que el confinamiento animó a muchas personas a considerar la bolsa como un a fuente extra de ingresos. Este hecho puede deberse a que al mismo tiempo de que muchas personas voltearon a ver esta alternativa, muchas otras vieron en los mercados financieros mucha incertudumbre. Por lo que el efecto se pudo haber neteado por ambos casos de personas.Esta suposición la puede respaldar el hecho de que, aunque en promedio el nivel de búsquedas no fue muy significativo, la varianza del nivel de búsquedas si aumentó. Por lo tanto, aumentó la volatilidad en las búsquedas. Que haya habido más volatilidad en las búsquedas defiende también nuestra hipótesis de que más gente se interesó en la bolsa y otras personas la dejaron de ver como alternativa para aumentar sus ingresos.

## Apéndice de contribuciones 
Miguel y Anahí investigaron los términos de búsqueda que se utilizaron para construir la base de datos. Miguel hizo todo el código para bajar la base de datos de Google Trends y plantear el DiD y la regresión discontinúa. Anahí se encargó de crear el blog y darle todo el diseño y estructura. Alejandro Gómez se encargo de describir la metodología utilizada y dar interpretación a los resultados. Miguel realizó las gráficas junto con Anahí y ayudaron todos a las conclusiones. Alejandro Ortiz no participó en el trabajo de manera activa. 
