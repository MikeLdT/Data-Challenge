---
title: "GEN Z, sin miedo a invertir"
author: "Miguel Lerdo de Tejada, Anahí Plascencia, Alejandro Gómez, Alejandro Ortiz"
date: "`r Sys.Date()`"
output:
  prettydoc::html_pretty:
    theme: architect
    highlight: github
---
<style>
body {
text-align: justify}

#TOC {
  color:#708090
  font-family: verdana;
  font-size: 16px
  border-color: #708090;
}
body {
  color: #708090;
  font-family: verdana;
  background-color: #F5F5F5;
}
</style>

# Cómo el COVID-19 ha sido un impulso para que los más jóvenes inviertan
La generación Z ha demostrado que nunca es demasiado pronto para empezar a invertir, desde que comenzó la pandemia en marzo de 2020 muchos jóvenes al rededor del mundo han decidido tomar las riendas de sus finanzas personales. Muchos quizá por aburrimiento, y otros por la frustración de lo que está pasando con la economía mundial han decidido intentarlo y los resultados han sido bastante interesantes. 

La facilidad con la que uno puede comprar acciones con un simple swipe a la izquierda es fascinante. El mundo digital ha venido a revolucionar los mercados y la Gen Z no perderá ninguna oportunidad para hacerlo desde casa. 

Por esta razón hemos decidido analizar los efectos del lockdown en Estados Unidos para descubrir si existe un efecto causal en la búsqueda de palabras relacionadas al mercado accionario utilizando Google Trends. Buscamos probar que debido a los confinamientos el número de búsquedas relacionadas con inversiones han aumentado. 


```{r, echo=FALSE}
knitr::opts_chunk$set(error = TRUE)
library("dplyr")
library("ggplot2")
library("corrplot")
library("gtrendsR")
library(readr) 
library(purrr) 
library(crossmap)
library(lubridate)
library(fixest)
library(naniar)
library(future)
library(rdrobust)
library(fastDummies)
library(rddensity)
library("rmarkdown")
library("naniar")
library("knitr")
library("furrr")
library("prettydoc")
options(repos="https://cran.rstudio.com" )
install.packages("pscl", repos = "https://cran.rstudio.com") 
```
Los resultados que obtuvimos se estimaron a través del método de Diferencias en Diferencias (DiD) que compara las búsquedas relacionadas con finanzas en el 2020 antes y después de los lockdowns con las búsquedas en las mismas fechas en el 2019. Los datos que se tomaron fueron para los 51 estados de Estados Unidos y la selección de palabras de términos de búsqueda relacionadas a la categoría de finanzas en Google Trends fueron las siguientes: Stock, Stocks, Market, NYSE, Finance, Finances, Dow, NASDAQ, Business, Rate, Rates y S&P. Los datos de búsqueda por cada término se registraron diariamente para cada estado en ambos periodos de tiempo. 

De acuerdo con los resultados encontrados nos podemos dar cuenta que a partir de la pandemia la gente empezó a tener un interés por las finanzas que se ve reflejado en como los términos de búsqueda aumentaron a partir del lockdown. (Incluir los datos en términos de desviaciones). En el estado de blah se vio un aumento de ... desviaciones estándar. Tal término aumentó --- veces. Mencionar qué pasó con los términos más relevantes y cómo aumentó su búsqueda por el lockdown. 
Para obtener las tendencias de búsqueda diarias entre el 1 de enero de 2019 y el 10 de abril de 2020, descargamos los datos diarios entre el 1 de enero y el 10 de abril, tanto en 2019 como en 2020. Como los datos diarios en 2019 provienen de una solicitud separada a los datos diarios en 2020, los factores de escala utilizados para calcular la puntuación de 0 a 100 no son los mismos en los dos períodos. Por lo tanto, cambiamos la escala de las dos series para que sean comparables.

##Prodecimiento de escala:

Denotemos con Di, c, 2019 el número de búsquedas diarias en Google de un tema el día i en el país c, durante el período del 1 de enero de 2019 al 10 de abril de 2019, con un número análogo Di, c, 2020 para el período del 1 de enero de 2020 al 10 de abril de 2020. Estos datos se obtienen para cada día individual i y toman valores entre 0 y 100 para cada día durante el período considerado. Sin embargo, no se puede comparar directamente los números de 2019 y 2020 ya que su denominador (el número máximo de búsquedas durante un día en el período) no es el mismo. Para poder comparar estas cifras, cambiamos la escala de los datos diarios para cada período por las respectivas ponderaciones de interés de búsqueda de la semana que calculamos utilizando datos semanales que están disponibles continuamente durante todo el período entre el 1 de enero de 2019 y el 10 de abril de 2020.

Denotemos por Di, c, 2019-2020 el número reescalado de búsquedas diarias en Google para este tema el día i en el país c durante el período del 1 de enero de 2019 al 10 de abril de 2020. Este factor lo calculamos de la siguiente manera:

Primero calculamos los respectivos niveles de busqueda semanal de los temas de nuestro interés para todas las semanas entre el 1 de enero de 2019 y el 10 de abril de 2020. Tomamos los datos diarios desde el 1 de enero de 2019 hasta el 10 de abril de 2019 y calculamos el promedio de búsquedas semanales para cada tema en el país c sobre este período y lo denotamos “Di, c, 2019”. Luego realizamos el mismo ejercicio para el período del 1 de enero de 2020 al 10 de abril de 2020 y lo denotamos:  “Di, c, 2020”.
De igual manera sacamos el promedio del nivel de busqueda de cada tema para todo el period (es decir, desde el 1 de enero de 2019 hasta el 10 de abril de 2020), tambiénlo denotamos: “Di, c, 2019-2020”. 

De lo anterior poderamos el nivel de las busquedas semanales, wc2019 y wc2020:

 

Con estas ponderaciones ahora podemos cambiar la escala de los datos diarios para cada período multiplicando “Di, c, 2019” por wc2019 en 2019 y “Di, c, 2020” por wc2020 en 2020.
Obtenemos:

 

Por último, normalizamos estas cifras para obtener cifras entre 0 y 100 remplazando Di,c,2019−2020 por:


 


##Selección de la muestra:


Utilizamos la informsción que Google Trends nos proporciona sobre el historial de busqueda de las personas en Estados Unidos, no solo porque fue uno de los paises que con mayor rapidez impusieron restricciones de confinamiento, sino también porque la cultura de la inversión digital en la bolsa de valores es mucho más común, porque una major cantidad de personas tienen acceso a internet. De esta manera suavizamos un poco el sesgo que podría existir en nuestra muestra por la exlusión de personas que no tienen acceso a internet para nuestro ejercicio. Este sesgo es una de las desventajas que tiene utilizar Google Trends como fuente de datos, asi como el sesgo por edades, porque es mucho más común que la gente joven utilice internet para buscar maneras de invertir en la bolsa, mientras la gente de edad lo puede hacer por otro medio. Esto podría excluir de nuestro ejercicio a muchas personas mayores.

Sin embargo, utilizar Google Trends en nuestro trabajo tiene tambien ventajas; primero, las muestras que obtenemos son grandes, por lo que se elimina el sesgo por muestras pequeñas. Segundo, no adolece de sesgos como el efecto expectativa del observador o el sesgo del entrevistador, porque los usuarios no tienen incentivos a mentir en sus busquedas privadas. Y tercero, no hay una submuestra de entrevistado, por lo que no participa en nuestro ejercicio gente auto informada.

Método:

Para estimar los efectos del bloqueo en las búsquedas relacionadas con el la inversión en la bolsa, nos basamos en una estimación de Diferencia en Diferencia (DiD) que compara las búsquedas antes y después del bloqueo en 2020 con las búsquedas anteriores y posteriores a la misma fecha en 2019, esto garantiza que los cambios estacionales no afectan nuestro ejercicio porque comparamos las mismas fechas, en diferentes años. 

Escribimos el modelo de regresión de diferencias en diferencias para un tema W como:

 

Donde α refleja el efecto del bloqueo en las búsquedas de Google para el tema Wi, c en el día i en el país c. Ti,c es una variable dummy que toma el valor uno en los días posteriores a que se anunció el confinamiento y es cero en fechas anteriores. El año del bloqueo es el año i y corresponde a 2020. La variable Xi − 1, c controla el número  de nuevas muertes por COVID-19 por día por cada millón de habitantes en el país c. El modelo incluye efectos fijos del país, ρc, así como efectos fijos de año, semana y día que aparecen en el vector µ. 

Los errores estándar son sólidos y están agrupados a nivel de día. La suposición clave en nuestro ejercicio es que, en ausencia del confinamiento, el comportamiento de los usuarios de Google habrían evolucionado de la misma manera que en el año anterior al bloqueo, es decir, una suposición de tendencia común.


##Estimadores RDD (Regresión discontinua)

Para mostrar que hay una ruptura estructural inmediata causada por el confinamiento en las tendencias de busqueda de inversiones en la bolsa, también utilizaremos un diseño de regresión discontinua (RDD), que identifica rupturas en dos series paramétricas estimadas antes y después del confinamiento. Al igual que con las estimaciones de DiD, comparamos estas rupturas con las estimadas durante el mismo período en 2019.

Sea D la variable de ejecución, que se define como la distancia absoluta en días desde el anuncio de la orden de confinamienro; es negativo para los días anteriores y positivo para los días posteriores, mientras que la fecha del anuncio real o contrafactual se establece como día cero. El anuncio de bloqueo Ti,c se define como ya lo habíamos definido anteriormente. Por lo tanto, el modelo RDD-DiD se puede escribir de la siguiente manera:
 

Donde α' refleja el efecto que causó el confinamiento en las búsquedas de Google del tema Wi, c en el día i en el país c. f (Di, c) es una función polinomial de la distancia en días desde el anuncio del confinamiento que interactuó con la variable dummy de bloqueo Ti,c, para permitir diferentes efectos en ambos lados de las regresiones (antes y después).  Además, se incluyeron los mismos controles que en los modelos DID.

```{r}
#cargo los ids de los estados y DC
data("countries")
edos <- countries %>% 
  filter(country_code=="US") %>% 
  filter(!is.na(sub_code)) %>% 
  select(sub_code) %>% 
  pull() %>% 
  unique() %>% 
  head(51)

# Load your keywords list (.csv file) 
#cat("\n", file= file.choose(), append = TRUE)
kwlist <- readLines("palabras2.csv")

#cargo las fechas de lockdown y les pego el identificador de cada edo
fechas <- read.csv("fechasperronas.csv")
fechas$State <- toupper(fechas$State)
fechas <- fechas %>% 
  rename(name=State)

counts <- countries %>% 
  filter(country_code=="US") %>% 
  filter(!is.na(sub_code)) %>% 
  head(51)

fechas_join <- left_join(x=fechas,y=counts,by=c("name")) %>% 
  rename(geo=sub_code)

#jalo las muertes y las atraso un dia
muertes <- read.csv("all-states-history.csv") %>% 
  select(date,state,death) %>% 
  mutate(state=paste0("US-",state),date=as.Date(date)+1) %>% 
  rename(geo=state)

```


```{r bajarPalabras, eval=F, cache=TRUE}


kwlist <- kwlist[! kwlist==""]

 
# The function wrap all the arguments of the gtrendR::trends function and return only the interest_over_time (you can change that)
googleTrendsData <- function (keywords,country,time) { 
  
  # Set the geographic region, time span, Google product,... 
  # for more information read the official documentation https://cran.r-project.org/web/packages/gtrendsR/gtrendsR.pdf 
  #keywords <- kwlist
  #country <- "US" 
  channel <- 'web' 
  
  trends <- gtrends(keywords, 
                   gprop = channel,
                   geo = country,
                   time = time,
                   category=7) 
    
  Sys.sleep(7)
  
  results <- trends$interest_over_time 
  results$hits <- as.character(results$hits)
  return(results)
  } 
date <- c("2019-01-01 2019-04-10","2020-01-01 2020-04-10") 

# googleTrendsData function is executed over the cross product of kwlist,edos and date for daily data
# and the weekday and week variables are created

output <- future_xmap_dfr(.l = list(kwlist,edos,date),
                  .f = ~ googleTrendsData(..1,..2,..3),
                  .progress = T, .options = furrr_options(seed=NULL)) %>% 
  mutate(weekday=wday(as.Date(date)),week=week(as.Date(date)))
  
 
# Download the dataframe "output" as a .csv file 
write.csv(output, "download_diarias1.csv")

date <- c("2019-01-01 2020-04-10")

# googleTrendsData function is executed over the cross product of kwlist,edos and date for daily data
# and the weekday and week variables are created
output_week <- future_xmap_dfr(.l = list(kwlist, edos,date),
                  .f = ~ googleTrendsData(..1,..2,..3),
                  .progress = T,.options = furrr_options(seed=NULL)) 
 
# Download the dataframe "output" as a .csv file 
write.csv(output_week, "download_semanal1.csv")
```

```{r rescaling}
output <- read.csv("download_diarias.csv") %>% 
  mutate(year=year(date))
output_week <- read.csv("download_semanal.csv") %>% 
  mutate(year=year(date))



#convierto los hits de character a numeric
output$hits <- as.numeric(output$hits)
output <- output %>% 
  mutate(hits=ifelse(is.na(hits),0,hits))
output_week$hits <- as.numeric(output_week$hits)

# calculo el interes promedio por palabra del periodo 2019-10 abril 2020
period_mean <- output_week %>% 
  group_by(keyword,geo) %>%
  summarise(period_mean=mean(hits),.groups="keep") %>% 
  ungroup() 

# junto el calculo anterior con los datos
output <- left_join(x=output,y=period_mean,by=c("keyword","geo"))

# calculo el promedio por semana para los datos diarios por estado y palabra
# luego hago los pesos y corrijo y reescalo los hits
week_share <- output %>% 
  mutate(year=year(date)) %>% 
  group_by(keyword,geo,week,year) %>% 
  mutate(weights=period_mean/mean(hits)) %>% 
  mutate(hits_aux=hits*weights) %>% 
  mutate(hits_corrected=100*hits_aux/max(hits_aux)) %>% 
  mutate(hits_corrected=ifelse(is.na(hits_corrected),0,hits_corrected)) %>% 
  ungroup()





```


```{r treatment}

# a cada estado le pego la fecha en que anunciarion, entro en vigor y empezo en la primera ciudad el lockdown
# y vienen en otro formato entonces se los cambio

week_share_join <- left_join(x=week_share,y=fechas_join,by=c("geo"))
week_share_join$date <- as.Date(week_share_join$date)
week_share_join$Lockdown.announced <- as.Date(week_share_join$Lockdown.announced,"%d/%m/%Y")
week_share_join$Lockdown.effective <- as.Date(week_share_join$Lockdown.effective,"%d/%m/%Y")
week_share_join$X1st.city.county.lockdown.effective <- as.Date(week_share_join$X1st.city.county.lockdown.effective,"%d/%m/%Y")

# creo las variables de si estan despues del lockdown o no

week_share_join <- week_share_join %>% 
  mutate(treat_announced=ifelse(date>=Lockdown.announced,1,0))%>% 
  mutate(treat_announced=ifelse(is.na(treat_announced),0,treat_announced)) %>% 
  mutate(treat_effective=ifelse(date>=Lockdown.effective,1,0))%>% 
  mutate(treat_effective=ifelse(is.na(treat_effective),0,treat_effective)) %>%
  mutate(treat_1stcity=ifelse(date>=X1st.city.county.lockdown.effective,1,0)) %>% 
  mutate(treat_1stcity=ifelse(is.na(treat_1stcity),0,treat_1stcity))
  




```

```{r muertes}

# le pego las muertes a la base

base_final <- left_join(x=week_share_join,y=muertes,by=c("date","geo")) %>% 
  mutate(death=ifelse(is.na(death),0,death)) %>%
  mutate(year=year(date))





```


```{r dif}

#corro el dif in dif para cada palabra de la lista

difs <- map(kwlist,function(x){
  feols(hits_corrected ~ death + i(treat_announced,year,2019) | weekday + geo + week + year,filter(base_final,keyword==x),cluster=~date)
}
  )


```


```{r,results='asis'}
a <- etable(difs[1],tex = F)
kable(a,format = "html")
```

```{r rdd}
#plot
base_final_rdd <- base_final %>% 
  filter(!is.na(Lockdown.announced)) %>% 
  mutate(duration=as.integer(date-Lockdown.announced))

stock_2020 <- base_final_rdd %>% 
  filter(keyword=="stock",year==2020) %>% 
  dummy_cols(c("weekday","geo"),remove_first_dummy = T)

covs <- cbind.data.frame(stock_2020$death,
                         stock_2020[,(ncol(base_final)+1):ncol(stock_2020)])

b <- rdplot(y=stock_2020$hits_corrected,x=stock_2020$duration,c=0,p=10,x.lim = c(-30,20),covs = covs)

a <- rdrobust(y=stock_2020$hits_corrected,x=stock_2020$duration,c=0,p=3,covs=covs)

rdd <- rddensity(X=stock_2020$duration)
rdplotdensity(rdd,X=stock_2020$duration)
#

ggplot(stock_2020,aes(x=duration))+
  geom_histogram()

```

